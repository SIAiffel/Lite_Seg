{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:08.699291Z",
     "start_time": "2021-05-14T06:17:08.690384Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader #데이터 불러오기\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#훈련\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "#시각화\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:08.845611Z",
     "start_time": "2021-05-14T06:17:08.836754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:09.196845Z",
     "start_time": "2021-05-14T06:17:09.064970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 7 20 23]\n",
      " [20 12 19]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def listFiles(rootdir='.', suffix='png'):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir\n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched as PNG or JPG\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n",
    "\n",
    "cityscapes_valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "class_map = dict(zip( range(19),cityscapes_valid_classes))\n",
    "\n",
    "def convertTrainIdToClassId(img):\n",
    "         temp=np.copy(img)\n",
    "         for trainID in range(19):\n",
    "             #print(trainID,\" \" ,class_map[trainID])\n",
    "             temp[img==trainID]=class_map[trainID]\n",
    "             \n",
    "         return temp\n",
    "\n",
    "\n",
    "def get_cityscapes_labels():\n",
    "    return np.array([\n",
    "         #[  0,   0,   0],\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32]\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    \"\"\"Encode segmentation label images as pascal classes\n",
    "    Args:\n",
    "        mask (np.ndarray): raw segmentation label image of dimension\n",
    "          (M, N, 3), in which the Pascal classes are encoded as colours.\n",
    "    Returns:\n",
    "        (np.ndarray): class map with dimensions (M,N), where the value at\n",
    "        a given location is the integer denoting the class index.\n",
    "    \"\"\"\n",
    "    mask = mask.astype(int)\n",
    "    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n",
    "    for ii, label in enumerate(get_cityscapes_labels()):#get_pascal_labels()\n",
    "        label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n",
    "    label_mask = label_mask.astype(int)\n",
    "    return label_mask\n",
    "\n",
    "\n",
    "def decode_seg_map_sequence(label_masks, dataset='pascal'):\n",
    "    rgb_masks = []\n",
    "    for label_mask in label_masks:\n",
    "        rgb_mask = decode_segmap(label_mask, dataset)\n",
    "        rgb_masks.append(rgb_mask)\n",
    "    rgb_masks = torch.from_numpy(np.array(rgb_masks).transpose([0, 3, 1, 2]))\n",
    "    return rgb_masks\n",
    "\n",
    "def decode_segmap(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal':\n",
    "      print()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    \n",
    "    r[label_mask == 255] = 0\n",
    "    g[label_mask == 255] = 0\n",
    "    b[label_mask == 255] =0\n",
    "    \n",
    "    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "    #replace blue with red as opencv uses bgr\n",
    "    rgb[:, :, 0] = r /255.0     \n",
    "    rgb[:, :, 1] = g /255.0\n",
    "    rgb[:, :, 2] = b /255.0\n",
    "#    \n",
    "#    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "#    #replace blue with red as opencv uses bgr\n",
    "#    rgb[:, :, 0] = b #/255.0     \n",
    "#    rgb[:, :, 1] = g #/255.0\n",
    "#    rgb[:, :, 2] = r #/255.0\n",
    "#    \n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "def decode_segmap_cv(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal':\n",
    "      print()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    \n",
    "    r[label_mask == 255] = 0\n",
    "    g[label_mask == 255] = 0\n",
    "    b[label_mask == 255] =0\n",
    "    \n",
    "#    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "#   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "#    #replace blue with red as opencv uses bgr\n",
    "#    rgb[:, :, 0] = r /255.0     \n",
    "#    rgb[:, :, 1] = g /255.0\n",
    "#    rgb[:, :, 2] = b /255.0\n",
    "#    \n",
    "    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "    #replace blue with red as opencv uses bgr\n",
    "    rgb[:, :, 0] = b #/255.0     \n",
    "    rgb[:, :, 1] = g #/255.0\n",
    "    rgb[:, :, 2] = r #/255.0\n",
    "#    \n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "def generate_param_report(logfile, param):\n",
    "    log_file = open(logfile, 'w')\n",
    "    for key, val in param.items():\n",
    "        log_file.write(key + ':' + str(val) + '\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "def lr_poly(base_lr, iter_, max_iter=100, power=0.9):\n",
    "    return base_lr * ((1 - float(iter_) / max_iter) ** power)\n",
    "\n",
    "\n",
    "    \n",
    "from torchvision import transforms \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print()\n",
    "    ar=np.array([[0,7,10],[7,3,6]])\n",
    "    z=convertTrainIdToClassId(ar)\n",
    "#    img3= transforms.ToPILImage()(torch.from_numpy(ou).type(torch.FloatTensor))#.detach().cpu()\n",
    "#    img3.save(oupath)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cityscapes code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:09.537952Z",
     "start_time": "2021-05-14T06:17:09.514807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from dataloader.utils import listFiles #모르겠음\n",
    "\n",
    "class Cityscapes(data.Dataset):\n",
    "\n",
    "    def __init__(self, root='path', split=\"train\", transform=None,extra=False):\n",
    "        \"\"\"\n",
    "        Cityscapes dataset folder has two folders, 'leftImg8bit' folder for images and 'gtFine_trainvaltest' \n",
    "        folder for annotated images with fine annotations 'labels'.\n",
    "        \"\"\"\n",
    "        self.root = os.getenv('HOME')+'/SIAIFFEL_Dataset/Cityscapes/'\n",
    "        self.split = split #train, validation, and test sets\n",
    "        self.transform = transform\n",
    "        self.files = {}\n",
    "        self.n_classes = 19\n",
    "        self.extra=extra\n",
    "\n",
    "        if not self.extra:\n",
    "            print(\"Using fine dataset\")\n",
    "            self.images_path = os.path.join(self.root, 'leftImg8bit_trainvaltest','leftImg8bit', self.split)\n",
    "            self.labels_path = os.path.join(self.root, 'gtFine_trainvaltest', 'gtFine', self.split)\n",
    "        else:\n",
    "            print(\"Using Coarse dataset\")\n",
    "\n",
    "            self.images_path = os.path.join(self.root, 'leftImg8bit', self.split)\n",
    "            self.labels_path = os.path.join(self.root, 'gtCoarse', 'gtCoarse', self.split)            \n",
    "            \n",
    "        #print(self.images_path)\n",
    "        self.files[split] = listFiles(rootdir=self.images_path, suffix='.png')#list of the pathes to images\n",
    "\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1] #not to train\n",
    "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self.class_names = ['road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
    "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
    "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
    "                            'motorcycle', 'bicycle']\n",
    "        \n",
    "        self.ignore_index = 255\n",
    "        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n",
    "        #print(self.class_map)\n",
    "        \n",
    "        if not self.files[split]:\n",
    "            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images.path))\n",
    "\n",
    "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files[self.split])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.files[self.split][index].rstrip()\n",
    "        #print(image_path)\n",
    "        if not self.extra:\n",
    "            label_path = os.path.join(self.labels_path,\n",
    "                                image_path.split(os.sep)[-2],\n",
    "                                os.path.basename(image_path)[:-15] + 'gtFine_labelIds.png')\n",
    "        else:\n",
    "            label_path = os.path.join(self.labels_path,\n",
    "                                image_path.split(os.sep)[-2],\n",
    "                                os.path.basename(image_path)[:-15] + 'gtCoarse_labelIds.png')\n",
    "        _img = Image.open(image_path).convert('RGB')\n",
    "        _tmp = np.array(Image.open(label_path), dtype=np.uint8)\n",
    "        _tmp = self.encode_segmap(_tmp)\n",
    "\n",
    "        _target = Image.fromarray(_tmp)\n",
    "\n",
    "        sample = {'image': _img, 'label': _target}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def encode_segmap(self, mask):\n",
    "        # Put all void classes to ignore_index\n",
    "        for _voidc in self.void_classes:\n",
    "            mask[mask == _voidc] = self.ignore_index\n",
    "        for _validc in self.valid_classes:\n",
    "            mask[mask == _validc] = self.class_map[_validc]\n",
    "        return mask\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:09.668302Z",
     "start_time": "2021-05-14T06:17:09.650102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fine dataset\n",
      "Found 2975 train images\n"
     ]
    }
   ],
   "source": [
    "abcd=Cityscapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:12.366801Z",
     "start_time": "2021-05-14T06:17:12.360525Z"
    }
   },
   "outputs": [],
   "source": [
    "#ad = abcd.encode_segmap(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:14.055033Z",
     "start_time": "2021-05-14T06:17:14.011805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        i=0\n",
    "        for i,l in enumerate( self.features[:4]):\n",
    "            #print(str(i),x.size())\n",
    "            x = l(x)\n",
    "        keep = x\n",
    "        for z,l in enumerate(self.features[4:]):\n",
    "            #print(str(z+i+1),x.size())\n",
    "            x = l(x)\n",
    "        return x,keep\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기가 daspp-concate-1*1cov-upsample(billinear)-concat ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T06:17:15.706453Z",
     "start_time": "2021-05-14T06:17:15.461989Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-58fe3be9b768>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone_networks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMobileNetV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maspp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparableconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeparableConv2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from models.backbone_networks import MobileNetV2\n",
    "from models import aspp\n",
    "from models.separableconv import SeparableConv2d \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RT(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=19,PRETRAINED_WEIGHTS=\".\", pretrained=True):\n",
    "        \n",
    "        super(RT, self).__init__()\n",
    "        print(\"LiteSeg-MobileNet...\")\n",
    "\n",
    "        self.mobile_features=MobileNetV2.MobileNetV2()\n",
    "        if pretrained:\n",
    "            state_dict = torch.load(PRETRAINED_WEIGHTS)\n",
    "            self.mobile_features.load_state_dict(state_dict)\n",
    "        \n",
    "        rates = [1, 3, 6, 9]\n",
    "\n",
    "\n",
    "        self.aspp1 = aspp.ASPP(1280, 96, rate=rates[0])\n",
    "        self.aspp2 = aspp.ASPP(1280, 96, rate=rates[1])\n",
    "        self.aspp3 = aspp.ASPP(1280, 96, rate=rates[2])\n",
    "        self.aspp4 = aspp.ASPP(1280, 96, rate=rates[3])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             nn.Conv2d(1280, 96, 1, stride=1, bias=False),\n",
    "                                             nn.BatchNorm2d(96),\n",
    "                                             nn.ReLU())\n",
    "        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n",
    "        self.conv1 =SeparableConv2d(480+1280,96,1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        #adopt [1x1, 48] for channel reduction.\n",
    "        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "        self.last_conv = nn.Sequential(#nn.Conv2d(24+96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(24+96,96,3,1,1),\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       #nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(96,96,3,1,1),\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(96, n_classes, kernel_size=1, stride=1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.mobile_features(input)\n",
    "        #print(x.size())\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1)\n",
    "        #print('after aspp cat',x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n",
    "                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)\n",
    "       # ablation=torch.max(low_level_features, 1)[1]\n",
    "        #print('after con on aspp output',x.size())\n",
    "\n",
    "        ##comment to remove low feature\n",
    "        #low_level_features = self.conv2(low_level_features)\n",
    "        #low_level_features = self.bn2(low_level_features)\n",
    "        #low_level_features = self.relu(low_level_features)\n",
    "        #print(\"low\",low_level_features.size())\n",
    "        \n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "        #print('after cat low feature with output of aspp',x.size())\n",
    "\n",
    "        x = self.last_conv(x)\n",
    "        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        return x#,ablation\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
