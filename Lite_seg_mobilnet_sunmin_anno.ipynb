{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader #데이터 불러오기\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#훈련\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "#시각화\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 7 20 23]\n",
      " [20 12 19]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def listFiles(rootdir='.', suffix='png'):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir\n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched as PNG or JPG\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n",
    "\n",
    "cityscapes_valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "class_map = dict(zip( range(19),cityscapes_valid_classes))\n",
    "\n",
    "def convertTrainIdToClassId(img):\n",
    "         temp=np.copy(img)\n",
    "         for trainID in range(19):\n",
    "             #print(trainID,\" \" ,class_map[trainID])\n",
    "             temp[img==trainID]=class_map[trainID]\n",
    "             \n",
    "         return temp\n",
    "\n",
    "\n",
    "def get_cityscapes_labels():\n",
    "    return np.array([\n",
    "         #[  0,   0,   0],\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32]\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    \"\"\"Encode segmentation label images as pascal classes\n",
    "    Args:\n",
    "        mask (np.ndarray): raw segmentation label image of dimension\n",
    "          (M, N, 3), in which the Pascal classes are encoded as colours.\n",
    "    Returns:\n",
    "        (np.ndarray): class map with dimensions (M,N), where the value at\n",
    "        a given location is the integer denoting the class index.\n",
    "    \"\"\"\n",
    "    mask = mask.astype(int)\n",
    "    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n",
    "    for ii, label in enumerate(get_cityscapes_labels()):#get_pascal_labels()\n",
    "        label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n",
    "    label_mask = label_mask.astype(int)\n",
    "    return label_mask\n",
    "\n",
    "\n",
    "def decode_seg_map_sequence(label_masks, dataset='pascal'):\n",
    "    rgb_masks = []\n",
    "    for label_mask in label_masks:\n",
    "        rgb_mask = decode_segmap(label_mask, dataset)\n",
    "        rgb_masks.append(rgb_mask)\n",
    "    rgb_masks = torch.from_numpy(np.array(rgb_masks).transpose([0, 3, 1, 2]))\n",
    "    return rgb_masks\n",
    "\n",
    "def decode_segmap(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal':\n",
    "      print()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    \n",
    "    r[label_mask == 255] = 0\n",
    "    g[label_mask == 255] = 0\n",
    "    b[label_mask == 255] =0\n",
    "    \n",
    "    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "    #replace blue with red as opencv uses bgr\n",
    "    rgb[:, :, 0] = r /255.0     \n",
    "    rgb[:, :, 1] = g /255.0\n",
    "    rgb[:, :, 2] = b /255.0\n",
    "#    \n",
    "#    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "#    #replace blue with red as opencv uses bgr\n",
    "#    rgb[:, :, 0] = b #/255.0     \n",
    "#    rgb[:, :, 1] = g #/255.0\n",
    "#    rgb[:, :, 2] = r #/255.0\n",
    "#    \n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "def decode_segmap_cv(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal':\n",
    "      print()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    \n",
    "    r[label_mask == 255] = 0\n",
    "    g[label_mask == 255] = 0\n",
    "    b[label_mask == 255] =0\n",
    "    \n",
    "#    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "#   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "#    #replace blue with red as opencv uses bgr\n",
    "#    rgb[:, :, 0] = r /255.0     \n",
    "#    rgb[:, :, 1] = g /255.0\n",
    "#    rgb[:, :, 2] = b /255.0\n",
    "#    \n",
    "    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "    #replace blue with red as opencv uses bgr\n",
    "    rgb[:, :, 0] = b #/255.0     \n",
    "    rgb[:, :, 1] = g #/255.0\n",
    "    rgb[:, :, 2] = r #/255.0\n",
    "#    \n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "def generate_param_report(logfile, param):\n",
    "    log_file = open(logfile, 'w')\n",
    "    for key, val in param.items():\n",
    "        log_file.write(key + ':' + str(val) + '\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "def lr_poly(base_lr, iter_, max_iter=100, power=0.9):\n",
    "    return base_lr * ((1 - float(iter_) / max_iter) ** power)\n",
    "\n",
    "\n",
    "    \n",
    "from torchvision import transforms \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print()\n",
    "    ar=np.array([[0,7,10],[7,3,6]])\n",
    "    z=convertTrainIdToClassId(ar)\n",
    "#    img3= transforms.ToPILImage()(torch.from_numpy(ou).type(torch.FloatTensor))#.detach().cpu()\n",
    "#    img3.save(oupath)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cityscapes code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from dataloader.utils import listFiles #모르겠음\n",
    "\n",
    "class Cityscapes(data.Dataset):\n",
    "\n",
    "    def __init__(self, root='path', split=\"train\", transform=None,extra=False):\n",
    "        \"\"\"\n",
    "        Cityscapes dataset folder has two folders, 'leftImg8bit' folder for images and 'gtFine_trainvaltest' \n",
    "        folder for annotated images with fine annotations 'labels'.\n",
    "        \"\"\"\n",
    "        self.root = os.getenv('HOME')+'/aiffel/0siaiffel_project/path/lite_data/'\n",
    "        self.split = split #train, validation, and test sets\n",
    "        self.transform = transform\n",
    "        self.files = {}\n",
    "        self.n_classes = 19\n",
    "        self.extra=extra\n",
    "\n",
    "        if not self.extra:\n",
    "            print(\"Using fine dataset\")\n",
    "            self.images_path = os.path.join(self.root, 'leftImg8bit(real)', self.split)\n",
    "            self.labels_path = os.path.join(self.root, 'gtFine(seg)', self.split)\n",
    "        else:\n",
    "            print(\"Using Coarse dataset\")\n",
    "\n",
    "            self.images_path = os.path.join(self.root, 'leftImg8bit', self.split)\n",
    "            self.labels_path = os.path.join(self.root, 'gtCoarse', 'gtCoarse', self.split)            \n",
    "            \n",
    "        #print(self.images_path)\n",
    "        self.files[split] = listFiles(rootdir=self.images_path, suffix='.png')#list of the pathes to images\n",
    "\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1] #not to train\n",
    "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self.class_names = ['road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
    "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
    "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
    "                            'motorcycle', 'bicycle']\n",
    "        \n",
    "        self.ignore_index = 255\n",
    "        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n",
    "        #print(self.class_map)\n",
    "        \n",
    "        if not self.files[split]:\n",
    "            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images.path))\n",
    "\n",
    "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files[self.split])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.files[self.split][index].rstrip()\n",
    "        #print(image_path)\n",
    "        if not self.extra:\n",
    "            label_path = os.path.join(self.labels_path,\n",
    "                                image_path.split(os.sep)[-2],\n",
    "                                os.path.basename(image_path)[:-15] + 'gtFine_labelIds.png')\n",
    "        else:\n",
    "            label_path = os.path.join(self.labels_path,\n",
    "                                image_path.split(os.sep)[-2],\n",
    "                                os.path.basename(image_path)[:-15] + 'gtCoarse_labelIds.png')\n",
    "        _img = Image.open(image_path).convert('RGB')\n",
    "        _tmp = np.array(Image.open(label_path), dtype=np.uint8)\n",
    "        _tmp = self.encode_segmap(_tmp)\n",
    "\n",
    "        _target = Image.fromarray(_tmp)\n",
    "\n",
    "        sample = {'image': _img, 'label': _target}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def encode_segmap(self, mask):\n",
    "        # Put all void classes to ignore_index\n",
    "        for _voidc in self.void_classes:\n",
    "            mask[mask == _voidc] = self.ignore_index\n",
    "        for _validc in self.valid_classes:\n",
    "            mask[mask == _validc] = self.class_map[_validc]\n",
    "        return mask\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fine dataset\n",
      "Found 2975 train images\n"
     ]
    }
   ],
   "source": [
    "abcd=Cityscapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from models.backbone_networks import MobileNetV2\n",
    "from models import aspp # daspp용으로 사용\n",
    "from models.separableconv import SeparableConv2d #depwise seperable conv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RT(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=19,PRETRAINED_WEIGHTS=\".\", pretrained=True):\n",
    "        \n",
    "        super(RT, self).__init__()\n",
    "        print(\"LiteSeg-MobileNet...\")\n",
    "\n",
    "        self.mobile_features=MobileNetV2.MobileNetV2() # 백본 모델 \n",
    "        if pretrained:\n",
    "            state_dict = torch.load(PRETRAINED_WEIGHTS)\n",
    "            self.mobile_features.load_state_dict(state_dict)\n",
    "        \n",
    "        rates = [1, 3, 6, 9] #aspp rate \n",
    "\n",
    "\n",
    "        self.aspp1 = aspp.ASPP(1280, 96, rate=rates[0]) #1280 (in_channels) 96(out_channels)\n",
    "        self.aspp2 = aspp.ASPP(1280, 96, rate=rates[1])\n",
    "        self.aspp3 = aspp.ASPP(1280, 96, rate=rates[2])\n",
    "        self.aspp4 = aspp.ASPP(1280, 96, rate=rates[3])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), #fc 연산 대체 \n",
    "                                             nn.Conv2d(1280, 96, 1, stride=1, bias=False),\n",
    "                                             nn.BatchNorm2d(96),\n",
    "                                             nn.ReLU())\n",
    "        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n",
    "        self.conv1 =SeparableConv2d(480+1280,96,1) # input채널에 480 더하고 separable conv실행 \n",
    "        self.bn1 = nn.BatchNorm2d(96) \n",
    "\n",
    "        #adopt [1x1, 48] for channel reduction.----??  이게 뭐지\n",
    "        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False) #여기서 들어온 24랑 output채널 96을 더한다. 왜?\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "     # 이것도 3*3aspp하고 난후 3*3 같음 (근데 3*3conv가 하나 없다. 24+96에 두개가 있는건가?)\n",
    "        self.last_conv = nn.Sequential(#nn.Conv2d(24+96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(24+96,96,3,1,1), #3*3 conv\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       #nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(96,96,3,1,1), #3*3conv\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),    #n classes -19\n",
    "                                       nn.Conv2d(96, n_classes, kernel_size=1, stride=1)) #1*1conv (average뒤에)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.mobile_features(input)\n",
    "        #print(x.size())\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)                                   #x.size()[1:] ->배치dim제외하고 all dim이란 의미 2는 뭐지?\n",
    "        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1) #이건 aspp의 output+high dim = concat 1\n",
    "        #print('after aspp cat',x.size())\n",
    "        x = self.conv1(x) #1*1 conv\n",
    "        x = self.bn1(x) \n",
    "        x = self.relu(x)\n",
    "        #upsampling*8(아래 계산이 8배 하는 수식인가?)\n",
    "        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n",
    "                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True) #upsampling*8\n",
    "       \n",
    "    # ablation=torch.max(low_level_features, 1)[1]\n",
    "        #print('after con on aspp output',x.size())\n",
    "\n",
    "        ##comment to remove low feature\n",
    "        #low_level_features = self.conv2(low_level_features)\n",
    "        #low_level_features = self.bn2(low_level_features)\n",
    "        #low_level_features = self.relu(low_level_features)\n",
    "        #print(\"low\",low_level_features.size())\n",
    "        \n",
    "        x = torch.cat((x, low_level_features), dim=1)  #upsampling한 x값 +백본 low에서 뽑은 feature= concat\n",
    "        #print('after cat low feature with output of aspp',x.size())\n",
    "\n",
    "        x = self.last_conv(x) #decoder(3*3 2개+1*1 1개)\n",
    "        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True) #upsamling*4\n",
    "\n",
    "        return x#,ablation\n",
    "\n",
    "    def freeze_bn(self): #앤뭐지?\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def __init_weight(self): #이건뭐지?\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
